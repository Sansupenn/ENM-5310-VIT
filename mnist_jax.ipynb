{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4zCO8QfN3cR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim import AdamW\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Hyperparameters\n",
        "exp_name = 'vit-with-10-epochs'\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "lr = 1e-5\n",
        "save_model_every = 0\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "config = {\n",
        "    \"patch_size\": 7,\n",
        "    \"hidden_size\": 64,\n",
        "    \"num_hidden_layers\": 4,\n",
        "    \"num_attention_heads\": 4,\n",
        "    \"intermediate_size\": 4 * 64,  # 4 * hidden_size\n",
        "    \"hidden_dropout_prob\": 0.0,\n",
        "    \"attention_probs_dropout_prob\": 0.0,\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"image_size\": 28,  # MNIST image size\n",
        "    \"num_classes\": 10,  # Number of classes in MNIST\n",
        "    \"num_channels\": 1,  # MNIST images are grayscale (1 channel)\n",
        "    \"qkv_bias\": True,\n",
        "    \"use_faster_attention\": True,\n",
        "}\n",
        "\n",
        "# Assert to make sure configurations are valid\n",
        "assert config[\"hidden_size\"] % config[\"num_attention_heads\"] == 0\n",
        "assert config['intermediate_size'] == 4 * config['hidden_size']\n",
        "assert config['image_size'] % config['patch_size'] == 0\n",
        "\n",
        "\n",
        "class ViTForClassification(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer (ViT) Model for Image Classification with L2 Regularization and Batch Normalization.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(ViTForClassification, self).__init__()\n",
        "        self.num_patches = (config['image_size'] // config['patch_size']) ** 2\n",
        "        self.embedding = nn.Linear(config['patch_size'] ** 2, config['hidden_size'])\n",
        "        self.attention = nn.MultiheadAttention(config['hidden_size'], config['num_attention_heads'])\n",
        "        self.fc = nn.Linear(config['hidden_size'], config['num_classes'])\n",
        "\n",
        "        # Batch normalization layers for the hidden_size dimension\n",
        "        self.batch_norm1 = nn.BatchNorm1d(config['hidden_size'])\n",
        "        self.batch_norm2 = nn.BatchNorm1d(config['hidden_size'])\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.3)  # 30% dropout for regularization\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, self.num_patches, -1)  # Flattening the patches\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Apply batch normalization after embedding (reshape to (batch_size, seq_len, hidden_size))\n",
        "        x = x.transpose(1, 2)  # Shape (batch_size, hidden_size, seq_len)\n",
        "        x = self.batch_norm1(x)  # Normalize over the hidden_size\n",
        "        x = x.transpose(1, 2)  # Transpose back to (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        x = self.dropout(x)  # Apply dropout after batch norm\n",
        "\n",
        "        # Apply attention (this is simplified for the example)\n",
        "        x, _ = self.attention(x, x, x)\n",
        "\n",
        "        # Batch normalization after attention\n",
        "        x = x.transpose(1, 2)  # Shape (batch_size, hidden_size, seq_len)\n",
        "        x = self.batch_norm2(x)\n",
        "        x = x.transpose(1, 2)  # Transpose back to (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        x = self.dropout(x)  # Apply dropout after batch norm\n",
        "        x = x.mean(dim=1)  # Pooling over sequence length (mean pooling)\n",
        "        logits = self.fc(x)\n",
        "        return logits, x\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"\n",
        "    The simple trainer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, optimizer, loss_fn, exp_name, device):\n",
        "        self.model = model.to(device)\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.exp_name = exp_name\n",
        "        self.device = device\n",
        "        self.train_losses = []\n",
        "        self.test_losses = []\n",
        "        self.accuracies = []\n",
        "\n",
        "    def train(self, trainloader, testloader, epochs, save_model_every_n_epochs=0):\n",
        "        \"\"\"\n",
        "        Train the model for the specified number of epochs.\n",
        "        \"\"\"\n",
        "        for i in range(epochs):\n",
        "            train_loss = self.train_epoch(trainloader)\n",
        "            accuracy, test_loss = self.evaluate(testloader)\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.test_losses.append(test_loss)\n",
        "            self.accuracies.append(accuracy)\n",
        "            print(f\"Epoch: {i+1}, Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "            if save_model_every_n_epochs > 0 and (i+1) % save_model_every_n_epochs == 0 and i+1 != epochs:\n",
        "                print('\\tSave checkpoint at epoch', i+1)\n",
        "                self.save_checkpoint(i+1)\n",
        "        self.plot_metrics()\n",
        "\n",
        "    def train_epoch(self, trainloader):\n",
        "        \"\"\"\n",
        "        Train the model for one epoch.\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        for batch in trainloader:\n",
        "            batch = [t.to(self.device) for t in batch]\n",
        "            images, labels = batch\n",
        "            self.optimizer.zero_grad()\n",
        "            loss = self.loss_fn(self.model(images)[0], labels)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            total_loss += loss.item() * len(images)\n",
        "        return total_loss / len(trainloader.dataset)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, testloader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in testloader:\n",
        "                batch = [t.to(self.device) for t in batch]\n",
        "                images, labels = batch\n",
        "                logits, _ = self.model(images)\n",
        "                loss = self.loss_fn(logits, labels)\n",
        "                total_loss += loss.item() * len(images)\n",
        "                predictions = torch.argmax(logits, dim=1)\n",
        "                correct += torch.sum(predictions == labels).item()\n",
        "        accuracy = correct / len(testloader.dataset)\n",
        "        avg_loss = total_loss / len(testloader.dataset)\n",
        "        return accuracy, avg_loss\n",
        "\n",
        "    def save_checkpoint(self, epoch):\n",
        "        checkpoint_dir = f\"./checkpoints/{self.exp_name}\"\n",
        "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f\"epoch_{epoch}.pth\")\n",
        "        torch.save(self.model.state_dict(), checkpoint_path)\n",
        "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "    def save_experiment(self, train_losses, test_losses, accuracies):\n",
        "        # Save experiment details, this can be enhanced as needed\n",
        "        print(\"Saving experiment details...\")\n",
        "        # Implement saving experiment results, model, etc.\n",
        "\n",
        "    def plot_metrics(self):\n",
        "        # Plot training loss, test loss, and accuracy vs epochs\n",
        "        epochs = range(1, len(self.train_losses) + 1)\n",
        "\n",
        "        # Plot loss vs epoch\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(epochs, self.train_losses, label='Train Loss')\n",
        "        plt.plot(epochs, self.test_losses, label='Test Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Train and Test Loss vs Epoch')\n",
        "        plt.legend()\n",
        "\n",
        "        # Plot accuracy vs epoch\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(epochs, self.accuracies, label='Accuracy', color='orange')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title('Accuracy vs Epoch')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def prepare_data(batch_size):\n",
        "    \"\"\"\n",
        "    Prepare MNIST dataset with the specified batch size and augmentations.\n",
        "    \"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomRotation(10),  # Random rotation of up to 10 degrees\n",
        "        transforms.RandomHorizontalFlip(),  # Random horizontal flip\n",
        "        transforms.RandomAffine(5),  # Random affine transformation (slight shift/scale)\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "    trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "\n",
        "def main():\n",
        "    trainloader, testloader = prepare_data(batch_size=batch_size)\n",
        "    model = ViTForClassification(config)\n",
        "\n",
        "    # Applying L2 regularization (weight decay) in optimizer\n",
        "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=1e-4)  # L2 regularization\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    trainer = Trainer(model, optimizer, loss_fn, exp_name, device=device)\n",
        "    trainer.train(trainloader, testloader, epochs, save_model_every_n_epochs=save_model_every)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    }
  ]
}