{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Implementation of ViT Model on MNIST Dataset using JAX**"
      ],
      "metadata": {
        "id": "VCuUK12MwqIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a Vision Transformer (ViT) model using JAX and Flax. It includes custom components such as a multi-head self-attention mechanism (SelfAttention) and feed-forward layers (dense_proj), organized into transformer blocks (TransformerBlock). The model processes input images by first dividing them into patches and embedding them into a higher-dimensional space (patch_embedding). The data then passes through multiple transformer blocks, each containing attention layers, feed-forward networks, layer normalization, and residual connections. The output is pooled and passed through a classification layer (ViTForClassification). The model is designed for image classification tasks, specifically applied to the MNIST dataset with built-in data augmentation techniques."
      ],
      "metadata": {
        "id": "Msc2QpDtwn-v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPdHKeXoyNSR",
        "outputId": "58b51fec-3a60-4c96-b82d-6b5e2728adba"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss = 1.9852, Test Loss = 1.5666, Accuracy = 0.4714\n",
            "Epoch 2: Train Loss = 1.4530, Test Loss = 1.2301, Accuracy = 0.5857\n",
            "Epoch 3: Train Loss = 1.2106, Test Loss = 1.0352, Accuracy = 0.6551\n",
            "Epoch 4: Train Loss = 1.0779, Test Loss = 0.9362, Accuracy = 0.6836\n",
            "Epoch 5: Train Loss = 0.9892, Test Loss = 0.8592, Accuracy = 0.7137\n",
            "Epoch 6: Train Loss = 0.9288, Test Loss = 0.8083, Accuracy = 0.7261\n",
            "Epoch 7: Train Loss = 0.8792, Test Loss = 0.7696, Accuracy = 0.7412\n",
            "Epoch 8: Train Loss = 0.8355, Test Loss = 0.7331, Accuracy = 0.7536\n",
            "Epoch 9: Train Loss = 0.8005, Test Loss = 0.7008, Accuracy = 0.7664\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from jax.random import PRNGKey\n",
        "\n",
        "# Hyperparameters\n",
        "exp_name = 'vit-with-10-epochs-jax'  # Experiment name\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "lr = 1e-5\n",
        "save_model_every = 0\n",
        "\n",
        "# Configuration\n",
        "config = {\n",
        "    \"patch_size\": 7,\n",
        "    \"hidden_size\": 64,\n",
        "    \"num_hidden_layers\": 4,\n",
        "    \"num_attention_heads\": 4,\n",
        "    \"intermediate_size\": 4 * 64,\n",
        "    \"hidden_dropout_prob\": 0.0,\n",
        "    \"attention_probs_dropout_prob\": 0.0,\n",
        "    \"image_size\": 28,\n",
        "    \"num_classes\": 10,\n",
        "    \"num_channels\": 1,\n",
        "}\n",
        "\n",
        "assert config[\"hidden_size\"] % config[\"num_attention_heads\"] == 0\n",
        "assert config['intermediate_size'] == 4 * config['hidden_size']\n",
        "assert config['image_size'] % config['patch_size'] == 0\n",
        "\n",
        "# Data preparation\n",
        "def prepare_data(batch_size):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomAffine(5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "    trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "# Transformer block\n",
        "class TransformerBlock(nn.Module):\n",
        "    config: dict\n",
        "\n",
        "    def setup(self):\n",
        "        self.attention = nn.SelfAttention(\n",
        "            num_heads=self.config['num_attention_heads'],\n",
        "            qkv_features=self.config['hidden_size'],\n",
        "            dropout_rate=self.config['attention_probs_dropout_prob']\n",
        "        )\n",
        "        self.dense_proj = nn.Sequential([\n",
        "            nn.Dense(self.config['intermediate_size']),\n",
        "            nn.gelu,\n",
        "            nn.Dense(self.config['hidden_size']),\n",
        "        ])\n",
        "        self.layernorm1 = nn.LayerNorm()\n",
        "        self.layernorm2 = nn.LayerNorm()\n",
        "        self.dropout = nn.Dropout(self.config['hidden_dropout_prob'])\n",
        "\n",
        "    def __call__(self, x, train):\n",
        "        attn_output = self.attention(self.layernorm1(x))\n",
        "        x = x + self.dropout(attn_output, deterministic=not train)\n",
        "        proj_output = self.dense_proj(self.layernorm2(x))\n",
        "        return x + self.dropout(proj_output, deterministic=not train)\n",
        "\n",
        "# Vision Transformer (ViT)\n",
        "class ViTForClassification(nn.Module):\n",
        "    config: dict\n",
        "\n",
        "    def setup(self):\n",
        "        self.num_patches = (self.config['image_size'] // self.config['patch_size']) ** 2\n",
        "        self.patch_embedding = nn.Dense(self.config['hidden_size'])\n",
        "        self.transformer_blocks = [\n",
        "            TransformerBlock(self.config) for _ in range(self.config['num_hidden_layers'])\n",
        "        ]\n",
        "        self.classifier = nn.Dense(self.config['num_classes'])\n",
        "        self.dropout = nn.Dropout(self.config['hidden_dropout_prob'])\n",
        "\n",
        "    def __call__(self, x, train=True):\n",
        "        batch_size = x.shape[0]\n",
        "        x = x.reshape(batch_size, self.num_patches, -1)\n",
        "        x = self.patch_embedding(x)\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, train)\n",
        "        x = x.mean(axis=1)  # Mean pooling\n",
        "        logits = self.classifier(x)\n",
        "        return logits\n",
        "\n",
        "# Training and evaluation utilities\n",
        "def create_train_state(rng, model, learning_rate):\n",
        "    params = model.init(rng, jnp.ones((batch_size, config['image_size'], config['image_size'], config['num_channels'])))['params']\n",
        "    tx = optax.adamw(learning_rate)\n",
        "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
        "\n",
        "def train_epoch(state, trainloader, loss_fn):\n",
        "    train_loss = 0\n",
        "    for batch in trainloader:\n",
        "        images, labels = batch\n",
        "        images = jnp.array(images.numpy())\n",
        "        labels = jnp.array(labels.numpy())\n",
        "\n",
        "        def loss_fn_wrapper(params):\n",
        "            logits = state.apply_fn({'params': params}, images, train=True)\n",
        "            return loss_fn(logits, labels)\n",
        "\n",
        "        loss, grads = jax.value_and_grad(loss_fn_wrapper)(state.params)\n",
        "        state = state.apply_gradients(grads=grads)\n",
        "        train_loss += loss * images.shape[0]\n",
        "\n",
        "    return state, train_loss / len(trainloader.dataset)\n",
        "\n",
        "def evaluate(state, testloader, loss_fn):\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for batch in testloader:\n",
        "        images, labels = batch\n",
        "        images = jnp.array(images.numpy())\n",
        "        labels = jnp.array(labels.numpy())\n",
        "        logits = state.apply_fn({'params': state.params}, images, train=False)\n",
        "        test_loss += loss_fn(logits, labels) * images.shape[0]\n",
        "        correct += (jnp.argmax(logits, axis=1) == labels).sum()\n",
        "\n",
        "    accuracy = correct / len(testloader.dataset)\n",
        "    avg_loss = test_loss / len(testloader.dataset)\n",
        "    return accuracy, avg_loss\n",
        "\n",
        "# Loss function\n",
        "def cross_entropy_loss(logits, labels):\n",
        "    one_hot_labels = jax.nn.one_hot(labels, config['num_classes'])\n",
        "    return -jnp.mean(jnp.sum(one_hot_labels * nn.log_softmax(logits), axis=-1))\n",
        "\n",
        "# Plot metrics\n",
        "def plot_metrics(train_losses, test_losses, accuracies):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, label='Train Loss')\n",
        "    plt.plot(epochs, test_losses, label='Test Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Train and Test Loss vs Epoch')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, accuracies, label='Accuracy', color='orange')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy vs Epoch')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    trainloader, testloader = prepare_data(batch_size=batch_size)\n",
        "    model = ViTForClassification(config)\n",
        "    rng = PRNGKey(0)\n",
        "    state = create_train_state(rng, model, lr)\n",
        "\n",
        "    train_losses, test_losses, accuracies = [], [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        state, train_loss = train_epoch(state, trainloader, cross_entropy_loss)\n",
        "        accuracy, test_loss = evaluate(state, testloader, cross_entropy_loss)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(test_loss)\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Test Loss = {test_loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "        if save_model_every > 0 and (epoch + 1) % save_model_every == 0:\n",
        "            checkpoint_dir = f\"./checkpoints/{exp_name}\"\n",
        "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "            checkpoint_path = os.path.join(checkpoint_dir, f\"epoch_{epoch + 1}.ckpt\")\n",
        "            with open(checkpoint_path, 'wb') as f:\n",
        "                f.write(flax.serialization.to_bytes(state))\n",
        "            print(f\"Checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "    plot_metrics(train_losses, test_losses, accuracies)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}